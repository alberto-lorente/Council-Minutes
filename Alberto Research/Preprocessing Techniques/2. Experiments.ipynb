{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzh65usQ4gyd",
        "outputId": "ab567c3b-13a1-44cf-c20e-c88dea57a5ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohappyeyeballs==2.4.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.4.4)\n",
            "Requirement already satisfied: aiohttp==3.11.11 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.11.11)\n",
            "Requirement already satisfied: aiosignal==1.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.7.0)\n",
            "Collecting anyio==4.8.0 (from -r requirements.txt (line 5))\n",
            "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting asttokens==3.0.0 (from -r requirements.txt (line 6))\n",
            "  Using cached asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: attrs==24.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (24.3.0)\n",
            "Requirement already satisfied: certifi==2024.12.14 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (2024.12.14)\n",
            "Requirement already satisfied: charset-normalizer==3.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (3.4.1)\n",
            "Collecting colorama==0.4.6 (from -r requirements.txt (line 10))\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting comm==0.2.2 (from -r requirements.txt (line 11))\n",
            "  Using cached comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting datasets==3.2.0 (from -r requirements.txt (line 12))\n",
            "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting debugpy==1.8.12 (from -r requirements.txt (line 13))\n",
            "  Using cached debugpy-1.8.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting decorator==5.1.1 (from -r requirements.txt (line 14))\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting dill==0.3.8 (from -r requirements.txt (line 15))\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting executing==2.1.0 (from -r requirements.txt (line 16))\n",
            "  Using cached executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: filelock==3.16.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (3.16.1)\n",
            "Requirement already satisfied: frozenlist==1.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (1.5.0)\n",
            "Collecting fsspec==2024.9.0 (from -r requirements.txt (line 19))\n",
            "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: h11==0.14.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (0.14.0)\n",
            "Requirement already satisfied: httpcore==1.0.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (1.0.7)\n",
            "Collecting httpx==0.27.2 (from -r requirements.txt (line 22))\n",
            "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub==0.27.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (0.27.1)\n",
            "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (3.10)\n",
            "Collecting ipykernel==6.29.5 (from -r requirements.txt (line 25))\n",
            "  Using cached ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting ipython==8.31.0 (from -r requirements.txt (line 26))\n",
            "  Using cached ipython-8.31.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting jedi==0.19.2 (from -r requirements.txt (line 27))\n",
            "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting Jinja2==3.1.3 (from -r requirements.txt (line 28))\n",
            "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting jupyter_client==8.6.3 (from -r requirements.txt (line 29))\n",
            "  Using cached jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: jupyter_core==5.7.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (5.7.2)\n",
            "Collecting MarkupSafe==2.1.5 (from -r requirements.txt (line 31))\n",
            "  Using cached MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (0.1.7)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (1.3.0)\n",
            "Requirement already satisfied: multidict==6.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (6.1.0)\n",
            "Collecting multiprocess==0.70.16 (from -r requirements.txt (line 35))\n",
            "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 36)) (1.6.0)\n",
            "Collecting networkx==3.2.1 (from -r requirements.txt (line 37))\n",
            "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting numpy==2.2.1 (from -r requirements.txt (line 38))\n",
            "  Using cached numpy-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting ollama==0.4.6 (from -r requirements.txt (line 39))\n",
            "  Using cached ollama-0.4.6-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: packaging==24.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (24.2)\n",
            "Collecting pandas==2.2.3 (from -r requirements.txt (line 41))\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Requirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (0.8.4)\n",
            "Requirement already satisfied: platformdirs==4.3.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 43)) (4.3.6)\n",
            "Collecting prompt_toolkit==3.0.49 (from -r requirements.txt (line 44))\n",
            "  Using cached prompt_toolkit-3.0.49-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: propcache==0.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 45)) (0.2.1)\n",
            "Collecting psutil==6.1.1 (from -r requirements.txt (line 46))\n",
            "  Using cached psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting pure_eval==0.2.3 (from -r requirements.txt (line 47))\n",
            "  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pyarrow==18.1.0 (from -r requirements.txt (line 48))\n",
            "  Using cached pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pydantic==2.10.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 49)) (2.10.5)\n",
            "Requirement already satisfied: pydantic_core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 50)) (2.27.2)\n",
            "Collecting Pygments==2.19.1 (from -r requirements.txt (line 51))\n",
            "  Using cached pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 52))\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: pytz==2024.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 53)) (2024.2)\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==308 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==308\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdlNS5bAENlC",
        "outputId": "de03d520-b19e-4951-ef81-a1b2e5e54bb5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from fr-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.10.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A5C0xPdi4gye"
      },
      "outputs": [],
      "source": [
        "with open(\"example_md_to_text.txt\", \"r\", encoding=\"latin-1\") as f: # TO DO: check proper encoding for .md files\n",
        "    markdown_example = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Gr2NScMJ4gye"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import tqdm\n",
        "# pprint(markdown_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMquaPnI4gye"
      },
      "source": [
        "Basic OLlama Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rmAT1Yjs4gye"
      },
      "outputs": [],
      "source": [
        "# import ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B0fIFS9d4gyf"
      },
      "outputs": [],
      "source": [
        "# # basic workflow of getting llama embeddings with ollama\n",
        "# ollama.embeddings(model=\"llama3.2:3b\",\n",
        "#                 prompt=\"Hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BV0isCso4gyf"
      },
      "outputs": [],
      "source": [
        "# # basic workflow of generating responses with ollama\n",
        "# ollama.generate(model=\"llama3.2:3b\",\n",
        "#                 prompt=\"Whos is Obama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ImuB4zw4gyf"
      },
      "source": [
        "Loading HF to get Attention Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CETuQGDb4gyf"
      },
      "outputs": [],
      "source": [
        "with open(\"HF_TOKEN.txt\", \"r\") as f:\n",
        "    hf_token = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsbEhjmB4gyf",
        "outputId": "13bbee92-aaa3-41e7-a0d3-50cb774ce7c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alberto-lorente\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfFolder, whoami\n",
        "\n",
        "HfFolder.save_token(hf_token)\n",
        "print(whoami()[\"name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFFEdmNL4gyg",
        "outputId": "c28e7bc1-54cd-4345-a0a4-d3d94f8093cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda available\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Cuda available\")\n",
        "  device = \"cuda:0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hS2YU_nD4gyg"
      },
      "outputs": [],
      "source": [
        "# using huggingface tokenizer for attettion layer\n",
        "# make sure you have GPU enabled\n",
        "# takes around 5 mins to load with TPUs\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_id = \"HIT-TMG/KaLM-embedding-multilingual-mini-v1\" # which model to use?\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
        "model = AutoModel.from_pretrained(model_id).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gppC6FUl92qV",
        "outputId": "78ac0d4c-d0e8-449c-8b6a-c0d4524a1d86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2Model(\n",
              "  (embed_tokens): Embedding(151936, 896)\n",
              "  (layers): ModuleList(\n",
              "    (0-23): 24 x Qwen2DecoderLayer(\n",
              "      (self_attn): Qwen2SdpaAttention(\n",
              "        (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "        (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "        (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "        (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "        (rotary_emb): Qwen2RotaryEmbedding()\n",
              "      )\n",
              "      (mlp): Qwen2MLP(\n",
              "        (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "        (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "        (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "        (act_fn): SiLU()\n",
              "      )\n",
              "      (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    )\n",
              "  )\n",
              "  (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "  (rotary_emb): Qwen2RotaryEmbedding()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRgyr7oN4gyg"
      },
      "source": [
        "- text enriching with VLLM to add a little description of tables before hey appear?\n",
        "\n",
        "- embeddings that return attention layer\n",
        "\n",
        "- perform similarity on the attention layer -> cluster similar sentences\n",
        "\n",
        "- summarize the clusters\n",
        "\n",
        "- tree-based graph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the text into sentences"
      ],
      "metadata": {
        "id": "0GgAqGk6DSVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method may take a lot of time. Alternatives?"
      ],
      "metadata": {
        "id": "ju4jHQE-Dv5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_sm\")"
      ],
      "metadata": {
        "id": "wKRiK15oDdgY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(markdown_example)\n",
        "sents = [sent.text for sent in doc.sents]"
      ],
      "metadata": {
        "id": "qn1F7NekDqc1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVcXDS4_EYwr",
        "outputId": "eb7d9d2b-e89f-4cde-a42c-055dec114d17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['# Séance publique\\n\\n\\n\\n# du jeudi 28 octobre 2021\\n\\n\\n\\n# à 18h00\\n\\n\\n\\n# Chorum',\n",
              " 'Alain Gilles',\n",
              " '- Halle Vacheresse\\n\\n\\n\\n#',\n",
              " 'Rue des Vernes à Roanne\\n\\n\\n\\n#',\n",
              " 'PROCES',\n",
              " 'VERBAL\\n\\n\\n\\n',\n",
              " \"L'an deux mille vingt et un, le 28 octobre à 18 h 00, les conseillers communautaires de Roannais Agglomération, se sont réunis à l\\x92Espace Chorum \\x96 Halle\",\n",
              " 'Vacheresse \\x96',\n",
              " 'Rue des Vernes à Roanne.\\n\\n\\n\\n',\n",
              " 'La convocation de tous les conseillers a été faite le 22 octobre 2021, dans les formes et délais prescrits par la loi, par Yves Nicolin, Président.\\n\\n\\n\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the text embeddings\n"
      ],
      "metadata": {
        "id": "I_QjccTUBASV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: There is one important caveat. We are computing sentence embeddings. If we were to pass the whole doc, the attention mask for each token would look very different."
      ],
      "metadata": {
        "id": "Lc1xqf1_IC-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example for one piece of text"
      ],
      "metadata": {
        "id": "T2IsSD5IEm62"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8grmXscv4gyg"
      },
      "outputs": [],
      "source": [
        "# dir(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oXyC3J8g4gyg"
      },
      "outputs": [],
      "source": [
        "tokenized_markdown = tokenizer(markdown_example, return_tensors=\"pt\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_markdown[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqfU1Njp_T0B",
        "outputId": "7f4fd116-b286-4f11-b342-d5ea96024b56"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20468"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mvm7hSEq4gyg"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  # first batch (only one since we are processing one doc)\n",
        "  # final token\n",
        "    embeddings = model(**tokenized_markdown)[0][:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaO-j34G_OeG",
        "outputId": "aceb90fa-07f0-4db5-e3ea-4f0318a4779d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 896])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take the last token since it's the one which contains all the aggregate info."
      ],
      "metadata": {
        "id": "Kfe2cySo_fGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings"
      ],
      "metadata": {
        "id": "r9Dz0XGjCnOC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CZN2u5nI4gyh"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "normalized_embeddings = F.normalize(embeddings, p=2, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalized_embeddings"
      ],
      "metadata": {
        "id": "75kj1dKJA4Uq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For a list of sentences"
      ],
      "metadata": {
        "id": "ZDRe1uk9Es4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_embeddings(sentence):\n",
        "\n",
        "    tokenized_sentences = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**tokenized_sentences)[0][:, 0]\n",
        "        # print(embeddings.shape)\n",
        "\n",
        "    normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "    detached_embeddings = normalized_embeddings.detach().cpu().numpy() # detached into cpu so that we can manipulate them for clustering\n",
        "\n",
        "    torch.cuda.empty_cache() # Careful with running out of memory\n",
        "\n",
        "    return detached_embeddings"
      ],
      "metadata": {
        "id": "OFOd5PChE7i_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detached_embeddings = compute_embeddings(sents[0])"
      ],
      "metadata": {
        "id": "8_SapulEFWyk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detached_sents_embeddings = [compute_embeddings(sent) for sent in sents]"
      ],
      "metadata": {
        "id": "nupBnyXyGD5a"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(detached_sents_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnUYDZj9Gbxv",
        "outputId": "6f8434d9-169c-48f6-877b-c713b4591a61"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "913"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(detached_sents_embeddings) == len(sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoDudDIDGeEv",
        "outputId": "956baa0e-4342-4d3f-b51d-5fa16f3dd53a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detached_sents_embeddings[0].shape"
      ],
      "metadata": {
        "id": "sjxS_QG8GiTj",
        "outputId": "7555baec-95df-4af8-98c1-5882ffa6e52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 896)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering"
      ],
      "metadata": {
        "id": "iSc1QSlJGloT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unsqueezeded_embeddings = [sent[0] for sent in detached_sents_embeddings] # there is an extra useless dimension"
      ],
      "metadata": {
        "id": "90kGMssB5sJq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unsqueezeded_embeddings[0:2]"
      ],
      "metadata": {
        "id": "QolXy-Qc6MRu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example for one number of clusters"
      ],
      "metadata": {
        "id": "r9-Od8pg7Idk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score"
      ],
      "metadata": {
        "id": "Z5bNPk2o7yHR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm = GaussianMixture(n_components=4, random_state=42)\n",
        "clusters = gm.fit_predict(unsqueezeded_embeddings)"
      ],
      "metadata": {
        "id": "W5XFk-ac5OEL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clusters # label assigned to each sentence"
      ],
      "metadata": {
        "id": "Hy--nqom6Z_l"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sil_sc = silhouette_score(unsqueezeded_embeddings, clusters)\n",
        "print(sil_sc) # the closer to 1 the better (how similar is an object to its cluster compared to the other clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF4VmocF6lgg",
        "outputId": "2557ce27-b0ff-447a-e59f-42df3b0dfa70"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.43139583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimal number of clusters"
      ],
      "metadata": {
        "id": "ZT9ds5aY7Kdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "range_clusters = np.arange(start=3, stop=9, step=1)\n",
        "print(range_clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reomAYOP7Okn",
        "outputId": "cdf2394b-bd8c-4c00-afdd-21417f3030c1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 4 5 6 7 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could compute jensenshannon distance as well."
      ],
      "metadata": {
        "id": "MQcSWEgK_4HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_n(n_clusters, embeddings):\n",
        "\n",
        "  gm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
        "  clusters = gm.fit_predict(embeddings)\n",
        "  sil_sc = silhouette_score(unsqueezeded_embeddings, clusters)\n",
        "\n",
        "  print(\"Number of clusters: \", n_clusters)\n",
        "  print(\"Score: \", sil_sc)\n",
        "  print()\n",
        "\n",
        "  return clusters, sil_sc"
      ],
      "metadata": {
        "id": "coeaWSg63QUT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_scores = []\n",
        "\n",
        "for n_cluster in range_clusters:\n",
        "  _, sil_sc = cluster_n(n_cluster, unsqueezeded_embeddings)\n",
        "  silhouette_scores.append(sil_sc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8LKJOTk4ZDi",
        "outputId": "acc60c0c-c007-4054-b99c-972b66fb7a2f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clusters:  3\n",
            "Score:  0.42344064\n",
            "\n",
            "Number of clusters:  4\n",
            "Score:  0.43139583\n",
            "\n",
            "Number of clusters:  5\n",
            "Score:  0.45841455\n",
            "\n",
            "Number of clusters:  6\n",
            "Score:  0.4844027\n",
            "\n",
            "Number of clusters:  7\n",
            "Score:  0.52773994\n",
            "\n",
            "Number of clusters:  8\n",
            "Score:  0.48493922\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max = np.argmax(silhouette_scores)\n",
        "optimal_n = range_clusters[max]\n",
        "print(\"Index\", max)\n",
        "print(\"Optimal Number of Clusters\", optimal_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLWh2rQz8RC2",
        "outputId": "80f8b05a-e013-4cdb-86c4-10b7edb5f0c1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 4\n",
            "Optimal Number of Clusters 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clusters, _ = cluster_n(optimal_n, unsqueezeded_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRylDnIF8lOy",
        "outputId": "5c4bf86f-3e28-49c0-a48d-c8da23f11b2d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clusters:  7\n",
            "Score:  0.52773994\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clusters"
      ],
      "metadata": {
        "id": "oTSMqiry82qz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logging Information, concatinating the text relating to each cluster and recomputing embeddings"
      ],
      "metadata": {
        "id": "8eIyvmFl9sdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sents[:4]"
      ],
      "metadata": {
        "id": "p9CjsPj49UjV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clusters[:4]"
      ],
      "metadata": {
        "id": "tvGQ1i929iyb"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unsqueezeded_embeddings[:4]"
      ],
      "metadata": {
        "id": "m_KkFnrW9cHZ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the indexes for the sentences belonging to each cluster\n",
        "\n",
        "dict_clusters_indexes = {}\n",
        "for cluster in range_clusters:\n",
        "  dict_clusters_indexes[cluster] = [] # the key is the cluster id\n",
        "  for index, sent_cluster in enumerate(clusters): # we are searching linearly so they should be sorted in ascending order already\n",
        "    if sent_cluster == cluster:\n",
        "      dict_clusters_indexes[cluster].append(index) # the list are the indexes that have that cluster id"
      ],
      "metadata": {
        "id": "BmQ_Ikdh9zni"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_clusters_indexes"
      ],
      "metadata": {
        "id": "Xhoc5s5i-Ydp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we have:\n",
        "- sents = the actual sentences split from SpaCy.\n",
        "- unsqueezed_embeddings = the embeddings of the sentences detached and unsqueezed.\n",
        "- clusters = the cluster each sentence belongs to.\n",
        "- dict_clusters_indexes = the indexes of the sentences of each cluster"
      ],
      "metadata": {
        "id": "0cPNekAS_KqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sents\n",
        "# unsqueezeded_embeddings\n",
        "# clusters\n",
        "# dict_clusters_indexes"
      ],
      "metadata": {
        "id": "wFBQ1yUr-_vY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_info = {}\n",
        "for index, sentence in enumerate(sents):\n",
        "\n",
        "  key = f\"sent_n_{index}\"\n",
        "  sentence_info[key] = {}\n",
        "  sentence_info[key][\"text\"] = sentence\n",
        "  sentence_info[key][\"embedding\"] = unsqueezeded_embeddings[index]\n",
        "  sentence_info[key][\"cluster\"] = clusters[index]\n",
        "  sentence_info[key][\"index\"] = index\n"
      ],
      "metadata": {
        "id": "l8oa6Jiq_iWG"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence_info['sent_n_0']"
      ],
      "metadata": {
        "id": "SudNSu9SApty"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_aggregates = {}\n",
        "for cluster in range(optimal_n):\n",
        "\n",
        "  # making a list of the dictionaries whose cluster matches the current cluster in the loop\n",
        "  list_dicts_sents = [dict_sent for dict_sent in sentence_info.values() if dict_sent[\"cluster\"] == cluster]\n",
        "\n",
        "  # making list of the texts, embeddings, indexes of all the sentences belonging to a cluster\n",
        "  list_texts = [dict_sent[\"text\"] for dict_sent in list_dicts_sents]\n",
        "  list_embeddings = [dict_sent[\"embedding\"] for dict_sent in list_dicts_sents]\n",
        "  list_indexes = [dict_sent[\"index\"] for dict_sent in list_dicts_sents]\n",
        "\n",
        "  cluster_aggregates[cluster] = {\"texts\": list_texts,\n",
        "                                \"embeddings\": list_embeddings,\n",
        "                                \"indexes\": list_indexes\n",
        "                                }"
      ],
      "metadata": {
        "id": "dsB-2RiuAxM9"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_aggregates.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TDlHvtXG4fD",
        "outputId": "975db732-a00d-47f6-ad66-a959605a340d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([0, 1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cluster_aggregates[5][\"texts\"][15:30]"
      ],
      "metadata": {
        "id": "9_uFUHf7CF5_"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: if we look at the text we can see that the clustering seems to group information of the same type but given that we got the embeddings for each sentence separately, computing global attention would probably leave us with better and more consistent sentence clusters. We can also augment the chunks by looking at start and end index of each sentence in the raw text and just adjusting a stride parameter. TO DO"
      ],
      "metadata": {
        "id": "2ahKsdjVCl4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sumarization"
      ],
      "metadata": {
        "id": "vQcVT6oq5E5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the basic separation wrt to clusters, we can start creating different representations for each cluster and doing the hierarchical indexing."
      ],
      "metadata": {
        "id": "eQfcKOHADIRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "summ_model = 'csebuetnlp/mT5_multilingual_XLSum'\n",
        "\n",
        "tokenizer_summ = AutoTokenizer.from_pretrained(summ_model)\n",
        "model_summ = AutoModelForSeq2SeqLM.from_pretrained(summ_model).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poc255oq5Gri",
        "outputId": "d1bff412-93ea-4e04-9a89-8acb88864785"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip())) # was part of the docs"
      ],
      "metadata": {
        "id": "QRtbKqB19P-3"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Individual example"
      ],
      "metadata": {
        "id": "fXOhgnCIAeJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_cluster = \" \".join(cluster_aggregates[5][\"texts\"])\n",
        "example_cluster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "yUvl3Ogu6l8-",
        "outputId": "bc9ef2dd-e582-4960-d5b1-fba8c55a0afe"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"PROCES Christine Aranéo Page 1 sur 49\\n\\n\\n\\n|Absents|Ni pouvoir|Suppléant|Pouvoir donné à\\x85|Ni suppléant|\\n\\n|---|---|---|---|---|\\n\\n|Jean -Marc Ambroise|X| Boyer| |Yves Meunier| -Marc Detour| Dupuis| Approbation des procès-verbaux des conseils communautaires des 24 juin et 22 juillet 2021 ASSEMBLEES\\n\\n\\n\\n# 1. Agglomération a délégué au président et au bureau communautaire des attributions.\\n\\n\\n\\n 2021 316 du 17 septembre 2021 et entretien d\\x92abris de bus et de mobiliers d\\x92information d\\x92Enseignement Supérieur en vue du regroupement des formations sur le Campus Mendes France à Roanne (Phase 2 : travaux de construction) avec la société ROCHE SARL ;\\n\\n 2021 319 du 20 septembre 2021 Abonnement FAS (Frais d\\x92Accès au Service) 2021 322 du 21 septembre 2021 CASSE 44, dont le siège est situé 3 ? rue de la Barbotte partenaire de CIDER Engineering ;\\n\\n Agglomération ;\\n\\n 2021 323 du 22 septembre 2021 n° 1\\n\\n\\n\\n BoOm » Délibérations prises par le Président\\n\\n\\n\\n# N° DP 2021-328 du 7 octobre 2021 Lot n°1 travaux de renouvellement et extension des réseaux de forte technicité » l\\x92augmentation du montant du marché de + 2 429, 60 \\x80 HT ;\\n\\n 2021 329 du 7 octobre 2021 2021 330 du 7 octobre 2021 Agglomération Agglomération, gestionnaire de l\\x92Aéroport de Roanne ;\\n\\n Délibérations prises par le Bureau Communautaire\\n\\n\\n\\n# situé sur le site aéroportuaire de Roanne-Bois de Pouilly - Commune de Saint-Léger-sur-Roanne Approuve la convention d\\x92occupation temporaire du domaine public aéroportuaire constitutive de droits réels avec la société SUPERBE AILE -Trot », d\\x92une surface de 400 m² justifiant d\\x92un accès aux pistes et contribuant au développement et à l\\x92animation du site aéroportuaire, notamment :\\n\\n\\n\\n- La formation dans le domaine du pilotage aérien, l\\x92instruction en vol, la fourniture de leçons de pilotage, et la réalisation de prestations d\\x92enseignement dans le domaine du pilotage aérien ;\\n\\n Autorise Agglomération sur le projet de modification n° 1 du Plan Local d\\x92Urbanisme de la commune de Notre-Dame-de-Boisset\\n\\n\\n\\n Concernant les dispositions relatives aux annexes à l\\x92habitation en zone A et N, emprise au sol et localisation par rapport à la maison d\\x92habitation, celles-ci ne devraient pas s\\x92appliquer aux piscines. Agglomération des biens précités ;\\n\\n\\n\\n Lucas LAPANDERY et Yannick PRAS\\n\\n\\n\\n décembre 2020 Episode de gel d\\x92avril 2021 Forez Roannais, Aux Racines de la Loire»\\n\\n\\n\\nLe bureau communautaire, après en avoir délibéré à l\\x92unanimité :\\n\\n\\n\\n Forez Roannais, Aux Racines de la Loire » Murcins des Grands Murcins » Profession dans le cadre de son activité de promotion de la gastronomie, et notamment sa participation à la soirée dégustation du festival Roanne Table Ouverte, qui aura lieu le lundi 4 octobre 2021 à Roanne, au titre de la promotion territoriale ;\\n\\n- précise que cette subvention est attribuée sous réserve de la réalisation de l\\x92événement ;\\n\\n Agglomération Agglomération comme suit :\\n\\n\\n\\n|N° et dénomination du lot|Attributaire(s) sous réserve transmission des PAA|Montant contractuel et rappel des montants minimum et maximum de l\\x92accord-cadre|Montant P2+P3 estimatif non-contractuel Agence LMTP\\n\\n\\n\\n Agence LMTP Agglomération et Ambierle, Mably, OPHEOR, Ouches, Pouilly-les-Nonains, Roannaise de l\\x92Eau, Renaison, Riorges, Saint-Martin-d\\x92Estreaux, Saint-Romain-La-Motte, Saint-Alban-les-Eaux, Saint-André-d\\x92Apchon et Villerest\\n\\n\\n\\n Agglomération et Ambierle, Mably, OPHEOR, Ouches, Pouilly-les-Nonains, Roannaise de l\\x92Eau, Renaison, Riorges, Saint-Martin-d\\x92Estreaux, Saint-Romain-La-Motte, Saint-Alban-les-Eaux, Saint-André-d\\x92Apchon et Villerest ;\\n\\n Annexe Année 2021\\n\\n\\n\\n Subvention à l\\x92association la Maison de Pays d\\x92Ambierle\\n\\n\\n\\n 2020 a été marquée par :\\n\\n\\n\\n- le renouvellement des exécutifs communaux (mars et juin) et intercommunal (juillet),\\n\\n- le COVID, avec la crise sanitaire et économique.\\n\\n\\n\\n Vu l\\x92arrêté préfectoral du 30 décembre 2019, portant statuts de Roannais Agglomération ;\\n\\n\\n\\n Vu l\\x92article L5211 39 du CGCT qui prévoit que le Président de l'établissement public de coopération intercommunale adresse chaque année Considérant que ledit rapport fait l'objet d'une communication, par le maire, au Conseil municipal en séance publique au cours de laquelle les représentants de la commune à l'organe délibérant de l'établissement public de coopération intercommunale sont entendus ;\\n\\n\\n\\nFranck Beysson fait plusieurs remarques sur ce rapport, et notamment sur l\\x92aéroport. Mais, il y avait cependant une volonté dans la rédaction justement de marquer un aspect écologique que l\\x92on trouve cocasse. J'ai quand même été un peu surpris de voir 0 fois le mot « gaz à effet de serre » biodiversité », mais deux fois le mot « flore », quatre fois le mot « climat », là où dans le même temps, il y avait 40 fois le mot « Mais, en ce qui concerne les haies, vous nous annoncez qu\\x92il y a 11,4 km de haies qui sont replantées. Mais, en tout cas d\\x92avoir l'information de comparaison serait intéressante pour pouvoir se donner des ordres de grandeur ».\\n\\n\\n\\nM. le Président répond à Franck Beysson : « Vous avez employé un mot qui est celui de cocasse. Mais, je ne suis pas sûr que cela fasse avancer les choses. Mais, franchement, cela ne me paraît pas très sérieux de réagir comme ça. Que nous n\\x92en fassions pas suffisamment, à votre goût, c'est certain et je ne suis pas là pour essayer de vous faire changer d'avis, ni d'opinion. Après, bien évidemment que l'aéroport consomme des carburants. Mais, Monsieur Beysson, chaque fois que vous mettez en marche votre micro, vous consommez de l'électricité, de l'énergie. Effectivement, l'aviation utilise des avions, ces avions brûlent du kérosène. Page 14 sur 49\\n\\n\\n\\nFranck Beysson répond « Mais, si vous prenez le bilan, par exemple de la ville de Grenoble, où nous avons aux affaires quelqu'un qui est proche des écologistes, en tout cas dans l'affichage, et que vous regardez les bilans de la qualité de l'air, entre 2014 et 2021, ça s'est largement dégradé. Jacques Troncy présente la constitution et reprise de provisions du compte épargne temps pour l\\x92année 2021.\\n\\n\\n\\n Vu le décret n° 2010-531 du 20 mai 2010 modifiant certaines dispositions relatives au compte épargne dans la fonction publique territoriale Agglomération doit reconnaître l\\x92engagement du CET dans son bilan et que cette dette est valorisée selon une méthode qui consiste à provisionner les jours accumulés sur le CET en les multipliant par le salaire journalier.\\n\\n\\n\\n Considérant qu\\x92ainsi, tous les jours inscrits sur les CET sont provisionnés, que la provision correspond au nombre de jours enregistrés dans le CET valorisés et qu\\x92à chaque fin d\\x92exercice Considérant qu\\x92au 31 décembre 2020 FIN D'EXERCICE 2021|\\n\\n|---|---|---|---|\\n\\n|BUDGET GENERAL|285 517|72 848|358 365|\\n\\n|BUDGET ASSAINISSEMENT|9 035|9 035|0|\\n\\n|BUDGET TOURISME ET LOISIRS\\n\\n\\n\\n# 688|720|968| |\\n\\n|---|---|---|---|---|\\n\\n|TOTAL|308 726|10 608|72 848|370 966|\\n\\n\\n\\n Jacques Troncy présente la création d\\x92une autorisation de programme pour la réorganisation de la collecte de déchets ménagers.\\n\\n\\n\\n Vu l\\x92arrêté préfectoral du 30 décembre 2019 portant statuts de Roannais Agglomération ;\\n\\n\\n\\n Vu le code Général des collectivités territoriales (CGCT) et notamment son article L2311-3 et R2311-9 ;\\n\\n\\n\\n Vu l\\x92instruction budgétaire et comptable M14 ;\\n\\n\\n\\nConsidérant que l'instruction budgétaire et comptable M14 précise que les Autorisations de Programme correspondent à des dépenses à caractère pluriannuel se rapportant à une immobilisation ou à un ensemble d'immobilisations déterminées, acquises ou réalisées par la collectivité ou à des subventions versées à des tiers ;\\n\\n\\n\\n Considérant que l\\x92autorisation de Programme (AP) constitue la limite supérieure des dépenses concourant à la réalisation d'un projet ou d'un ensemble de projets d'une même politique qui peuvent être engagées pour le financement des investissements ;\\n\\n\\n\\nConsidérant que les crédits de paiement (CP) constituent la limite supérieure des dépenses pouvant être mandatées durant l\\x92exercice, pour la couverture des engagements contractés dans le cadre des autorisations de programme correspondantes ;\\n\\n\\n\\n Considérant la volonté de Roannais Agglomération de réorganiser la collecte des déchets ménagers ;\\n\\n\\n\\n Jean-Yves Boire n\\x92a rien de particulier à ajouter, si ce n\\x92est le fait qu\\x92une délibération est également inscrite à cet ordre du jour pour relancer le premier gros marché relatif à l'approvisionnement en bacs. Franck Beysson profite de la présence de Jean-Yves Boire pour réitérer la question qu\\x92il avait posée lors du précédent conseil. Jean-Yves Boire confirme que les réflexions portant sur la collecte sont en cours et qu\\x92elles avancent bien. développer au maximum le compostage individuel ;\\n\\n\\n\\n# 2. Normalement, nous serons en ordre de marche, dès janvier 2023. Vous dire aujourd\\x92hui le nombre de composteurs individuels, collectifs, et la part qui sera réellement collectée en porte à porte est encore un petit peu tôt. Jean-Yves Boire répond que pour l\\x92instant quelque chose existe. Jean-Yves Boire ajoute qu\\x92il y aura encore beaucoup à faire et que c'est bien le but de cette usine de tri que de séparer les matériaux, de façon à les mettre dans les bonnes filières et que l'on puisse en recycler un maximum. déchets ménagers » sur le budget général pour un montant global de 9 000 000 \\x80 et la répartition prévisionnelle des crédits de paiement sur la période 2021-2025 comme suit :\\n\\n\\n\\n|Millésime|N° AP|Montant|CP 2021|CP 2022|CP 2023|CP 2024|CP 2025|\\n\\n|---|---|---|---|---|---|---|---|\\n\\n|2021|1040|9 000 000 \\x80|200 000 \\x80|5 600 000 \\x80|2 000 000 \\x80|600 000 \\x80|600 000 \\x80|\\n\\n\\n\\nIndique que les crédits de paiement (CP) de 2021 d\\x92un montant de 200 000 \\x80 seront inscrits à la prochaine Décision Modificative sur le Budget Général.\\n\\n\\n\\n# 5. Agglomération ainsi que des résultats de l\\x92exercice 2019 suite au transfert de la compétence eau de la commune des Noës à Roannais Agglomération qui seront reversés à la Roannaise de l\\x92eau. Augmentation Page 18 sur 49\\n\\n\\n\\n# Conservatoire 47 000 \\x80\\n\\n\\n\\nAjustement provision pour dépréciation des actifs circulants (-26 k\\x80) et provision pour Compte Epargne Temps (73 k\\x80).\\n\\n\\n\\n# Produits des services : Subventions\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_example = tokenizer_summ(\n",
        "                              [WHITESPACE_HANDLER(example_cluster)],\n",
        "                              return_tensors=\"pt\",\n",
        "                              padding=\"max_length\",\n",
        "                              truncation=True\n",
        "                          ).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iao1G9ap983n",
        "outputId": "1252c749-3eba-43f8-b11a-2beb9cf8eafd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAfEt4BU-ERE",
        "outputId": "5a477af3-cb2b-4823-a51b-c80e4ed46442"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   259, 167667,    259,  ...,  25645,    263,      1]],\n",
              "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenized_example[\"input_ids\"]\n",
        "# input_ids = tokenized_example[\"input_ids\"].squeeze(dim=0) NOP,expects the unsqueexed dimension"
      ],
      "metadata": {
        "id": "jgFcC-EI-eUB"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5_KUYMu-kSn",
        "outputId": "7d0ad17c-53d2-4498-c1e9-055a6787a87d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   259, 167667,    259,  ...,  25645,    263,      1]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At4p5Mn8_pMP",
        "outputId": "a1e45177-ee95-4c24-b21d-8142e3cfd13c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3030"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model_summ.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=1000,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_beams=4) # if it is too slow, adjust num_beams and max_len"
      ],
      "metadata": {
        "id": "sY34ss9r9W-P"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPFNUWGB-8Mp",
        "outputId": "da945120-a26b-4750-e2d7-ac68a1689a90"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([     0,   1760,   1888,  43159,    498,  23383,    263,    317, 162817,\n",
              "          7636,    269,    283,  16674,    265,    269,   3897,  22770,    259,\n",
              "           267,      1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = tokenizer_summ.decode(\n",
        "    output[0],\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")"
      ],
      "metadata": {
        "id": "RBfLlqnD-M6b"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dSHTZ2Th9o3X",
        "outputId": "df10c18d-b7ec-48f5-bde5-16f685083ac8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Les résultats des conseils communautaires de la commune de Roanne :'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For the clusters"
      ],
      "metadata": {
        "id": "WKovfmQhAh53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import autocast"
      ],
      "metadata": {
        "id": "mE3clzofFqwl"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(concat_text):\n",
        "\n",
        "  WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
        "\n",
        "  tokenized_example = tokenizer_summ(\n",
        "                              [WHITESPACE_HANDLER(concat_text)],\n",
        "                              return_tensors=\"pt\",\n",
        "                              padding=\"max_length\",\n",
        "                              truncation=True).to(device)\n",
        "\n",
        "  input_ids = tokenized_example[\"input_ids\"]\n",
        "\n",
        "  model_summ.eval()\n",
        "  with autocast(\"cuda\"):\n",
        "    summary_embedding_output = model_summ.generate(\n",
        "                                              input_ids=input_ids,\n",
        "                                              max_length=1000,\n",
        "                                              no_repeat_ngram_size=2,\n",
        "                                              num_beams=4) # if it is too slow, adjust num_beams and max_len\n",
        "\n",
        "  summary_decoded = tokenizer_summ.decode(\n",
        "                                    summary_embedding_output[0],\n",
        "                                    skip_special_tokens=True,\n",
        "                                    clean_up_tokenization_spaces=False)\n",
        "\n",
        "  torch.cuda.empty_cache() # Careful with running out of memory\n",
        "\n",
        "  return summary_embedding_output.detach().cpu().numpy(), summary_decoded"
      ],
      "metadata": {
        "id": "AMOBna6_5eFS"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "zT5TmgOkEZBj"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_cluster(cluster, cluster_aggregates=cluster_aggregates):\n",
        "\n",
        "  print(\"Procesing cluster \", cluster+1)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  cluster_aggregates[cluster][\"broader_representations\"] = {}\n",
        "\n",
        "  concat_text = \" \".join(cluster_aggregates[cluster][\"texts\"])\n",
        "  cluster_aggregates[cluster][\"broader_representations\"][\"concat_text\"] = concat_text\n",
        "\n",
        "\n",
        "  cluster_embeddings = compute_embeddings(concat_text)\n",
        "  cluster_aggregates[cluster][\"broader_representations\"][\"cluster_sentence_embeddings\"] = cluster_embeddings[0] # taking out the extra unnecesary dimension\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "  summary_embedding_output, summary_decoded = generate_summary(concat_text)\n",
        "  cluster_aggregates[cluster][\"broader_representations\"][\"cluster_summary_embedding\"] = summary_embedding_output\n",
        "  cluster_aggregates[cluster][\"broader_representations\"][\"cluster_summary_decoded\"] = summary_decoded\n",
        "\n",
        "  return cluster_aggregates\n"
      ],
      "metadata": {
        "id": "_RmBpsqmDgES"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cluster in list(cluster_aggregates.keys()):\n",
        "  cluster_aggregates = process_cluster(cluster)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "aapNTjqsFXs7",
        "outputId": "98ed740e-97b2-4956-f209-cd8ad954fa8b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesing cluster  1\n",
            "Procesing cluster  2\n",
            "Procesing cluster  3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 3.73 GiB. GPU 0 has a total capacity of 14.75 GiB of which 941.06 MiB is free. Process 181395 has 13.83 GiB memory in use. Of the allocated memory 13.52 GiB is allocated by PyTorch, and 183.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-fa6c0ebb5a28>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_aggregates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mcluster_aggregates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-71-70d818d5b36e>\u001b[0m in \u001b[0;36mprocess_cluster\u001b[0;34m(cluster, cluster_aggregates)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0msummary_embedding_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mcluster_aggregates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"broader_representations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cluster_summary_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_embedding_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mcluster_aggregates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"broader_representations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cluster_summary_decoded\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_decoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-d60280bd6786>\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(concat_text)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mmodel_summ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     summary_embedding_output = model_summ.generate(\n\u001b[0m\u001b[1;32m     16\u001b[0m                                               \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                               \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   2065\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mt5/modeling_mt5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 )\n\u001b[1;32m   1110\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1112\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mt5/modeling_mt5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     ):\n\u001b[0;32m--> 553\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mt5/modeling_mt5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    467\u001b[0m     ):\n\u001b[1;32m    468\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mt5/modeling_mt5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;31m# (batch_size, n_heads, seq_length, key_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 0 has a total capacity of 14.75 GiB of which 941.06 MiB is free. Process 181395 has 13.83 GiB memory in use. Of the allocated memory 13.52 GiB is allocated by PyTorch, and 183.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_aggregates[0][\"broader_representations\"].keys()"
      ],
      "metadata": {
        "id": "lH7rASoyHG4q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}