{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxcHZC7EaxIO"
      },
      "source": [
        "REQUIREMENTS.TXT\n",
        "EXAMPLE_MD_TO_TEXT.TXT\n",
        "HF_TOKEN.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"example_md_to_text.txt\", \"r\", encoding=\"latin-1\") as f: # TO DO: check proper encoding for .md files\n",
        "    markdown_example = f.read()\n",
        "\n",
        "with open(\"HF_TOKEN.txt\", \"r\") as f:\n",
        "    hf_token = f.read()"
      ],
      "metadata": {
        "id": "P_9551bjnMHb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzh65usQ4gyd",
        "outputId": "63190ba4-1ba1-410f-c2a1-45e07d2b1b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohappyeyeballs==2.4.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.4.4)\n",
            "Requirement already satisfied: aiohttp==3.11.11 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.11.11)\n",
            "Requirement already satisfied: aiosignal==1.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.7.0)\n",
            "Collecting anyio==4.8.0 (from -r requirements.txt (line 5))\n",
            "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting asttokens==3.0.0 (from -r requirements.txt (line 6))\n",
            "  Using cached asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting attrs==24.3.0 (from -r requirements.txt (line 7))\n",
            "  Using cached attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting certifi==2024.12.14 (from -r requirements.txt (line 8))\n",
            "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: charset-normalizer==3.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (3.4.1)\n",
            "Collecting colorama==0.4.6 (from -r requirements.txt (line 10))\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting comm==0.2.2 (from -r requirements.txt (line 11))\n",
            "  Using cached comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting datasets==3.2.0 (from -r requirements.txt (line 12))\n",
            "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting debugpy==1.8.12 (from -r requirements.txt (line 13))\n",
            "  Using cached debugpy-1.8.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting decorator==5.1.1 (from -r requirements.txt (line 14))\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting dill==0.3.8 (from -r requirements.txt (line 15))\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting executing==2.1.0 (from -r requirements.txt (line 16))\n",
            "  Using cached executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting filelock==3.16.1 (from -r requirements.txt (line 17))\n",
            "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: frozenlist==1.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (1.5.0)\n",
            "Collecting fsspec==2024.9.0 (from -r requirements.txt (line 19))\n",
            "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: h11==0.14.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (0.14.0)\n",
            "Requirement already satisfied: httpcore==1.0.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (1.0.7)\n",
            "Collecting httpx==0.27.2 (from -r requirements.txt (line 22))\n",
            "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub==0.27.1 (from -r requirements.txt (line 23))\n",
            "  Using cached huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (3.10)\n",
            "Collecting ipykernel==6.29.5 (from -r requirements.txt (line 25))\n",
            "  Using cached ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting ipython==8.31.0 (from -r requirements.txt (line 26))\n",
            "  Using cached ipython-8.31.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting jedi==0.19.2 (from -r requirements.txt (line 27))\n",
            "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting Jinja2==3.1.3 (from -r requirements.txt (line 28))\n",
            "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting jupyter_client==8.6.3 (from -r requirements.txt (line 29))\n",
            "  Using cached jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: jupyter_core==5.7.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (5.7.2)\n",
            "Collecting MarkupSafe==2.1.5 (from -r requirements.txt (line 31))\n",
            "  Using cached MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (0.1.7)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (1.3.0)\n",
            "Requirement already satisfied: multidict==6.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (6.1.0)\n",
            "Collecting multiprocess==0.70.16 (from -r requirements.txt (line 35))\n",
            "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 36)) (1.6.0)\n",
            "Collecting networkx==3.2.1 (from -r requirements.txt (line 37))\n",
            "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting numpy==2.2.1 (from -r requirements.txt (line 38))\n",
            "  Using cached numpy-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting ollama==0.4.6 (from -r requirements.txt (line 39))\n",
            "  Using cached ollama-0.4.6-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: packaging==24.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (24.2)\n",
            "Collecting pandas==2.2.3 (from -r requirements.txt (line 41))\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Requirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (0.8.4)\n",
            "Requirement already satisfied: platformdirs==4.3.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 43)) (4.3.6)\n",
            "Collecting prompt_toolkit==3.0.49 (from -r requirements.txt (line 44))\n",
            "  Using cached prompt_toolkit-3.0.49-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: propcache==0.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 45)) (0.2.1)\n",
            "Collecting psutil==6.1.1 (from -r requirements.txt (line 46))\n",
            "  Using cached psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting pure_eval==0.2.3 (from -r requirements.txt (line 47))\n",
            "  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pyarrow==18.1.0 (from -r requirements.txt (line 48))\n",
            "  Using cached pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting pydantic==2.10.5 (from -r requirements.txt (line 49))\n",
            "  Using cached pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pydantic_core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 50)) (2.27.2)\n",
            "Collecting Pygments==2.19.1 (from -r requirements.txt (line 51))\n",
            "  Using cached pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 52))\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz==2024.2 (from -r requirements.txt (line 53))\n",
            "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==308 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==308\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdlNS5bAENlC",
        "outputId": "40402f3a-b4f1-4ac4-bdc7-ce1fb92b5a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from fr-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "! python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Gr2NScMJ4gye"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import tqdm\n",
        "# pprint(markdown_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMquaPnI4gye"
      },
      "source": [
        "Basic OLlama Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rmAT1Yjs4gye"
      },
      "outputs": [],
      "source": [
        "# import ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B0fIFS9d4gyf"
      },
      "outputs": [],
      "source": [
        "# # basic workflow of getting llama embeddings with ollama\n",
        "# ollama.embeddings(model=\"llama3.2:3b\",\n",
        "#                 prompt=\"Hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BV0isCso4gyf"
      },
      "outputs": [],
      "source": [
        "# # basic workflow of generating responses with ollama\n",
        "# ollama.generate(model=\"llama3.2:3b\",\n",
        "#                 prompt=\"Whos is Obama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ImuB4zw4gyf"
      },
      "source": [
        "Loading HF to get Attention Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CETuQGDb4gyf"
      },
      "outputs": [],
      "source": [
        "with open(\"HF_TOKEN.txt\", \"r\") as f:\n",
        "    hf_token = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsbEhjmB4gyf",
        "outputId": "d6885c31-80b1-412f-a60f-349a0d3fdf30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alberto-lorente\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfFolder, whoami\n",
        "\n",
        "HfFolder.save_token(hf_token)\n",
        "print(whoami()[\"name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFFEdmNL4gyg",
        "outputId": "ece889cd-e17a-452c-c64e-ae09a21b0532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda available\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Cuda available\")\n",
        "  device = \"cuda:0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hS2YU_nD4gyg"
      },
      "outputs": [],
      "source": [
        "# using huggingface tokenizer for attettion layer\n",
        "# make sure you have GPU enabled\n",
        "# takes around 5 mins to load with TPUs\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_id = \"HIT-TMG/KaLM-embedding-multilingual-mini-v1\" # which model to use?\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
        "model = AutoModel.from_pretrained(model_id).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gppC6FUl92qV",
        "outputId": "81ca7103-cc07-437a-86df-67c3d504c56b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2Model(\n",
              "  (embed_tokens): Embedding(151936, 896)\n",
              "  (layers): ModuleList(\n",
              "    (0-23): 24 x Qwen2DecoderLayer(\n",
              "      (self_attn): Qwen2Attention(\n",
              "        (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "        (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "        (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "        (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "      )\n",
              "      (mlp): Qwen2MLP(\n",
              "        (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "        (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "        (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "        (act_fn): SiLU()\n",
              "      )\n",
              "      (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    )\n",
              "  )\n",
              "  (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "  (rotary_emb): Qwen2RotaryEmbedding()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRgyr7oN4gyg"
      },
      "source": [
        "- text enriching with VLLM to add a little description of tables before hey appear?\n",
        "\n",
        "- embeddings that return attention layer\n",
        "\n",
        "- perform similarity on the attention layer -> cluster similar sentences\n",
        "\n",
        "- summarize the clusters\n",
        "\n",
        "- tree-based graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GgAqGk6DSVO"
      },
      "source": [
        "## Splitting the text into sentences and paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wKRiK15oDdgY"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qn1F7NekDqc1"
      },
      "outputs": [],
      "source": [
        "doc = nlp(markdown_example)\n",
        "sents = [sent.text for sent in doc.sents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVcXDS4_EYwr",
        "outputId": "27d15748-3b35-4c27-e40d-481bbfe3a8bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['# Séance publique\\n\\n\\n\\n# du jeudi 28 octobre 2021\\n\\n\\n\\n# à 18h00\\n\\n\\n\\n# Chorum',\n",
              " 'Alain Gilles',\n",
              " '- Halle Vacheresse\\n\\n\\n\\n#',\n",
              " 'Rue des Vernes à Roanne\\n\\n\\n\\n#',\n",
              " 'PROCES',\n",
              " 'VERBAL\\n\\n\\n\\n',\n",
              " \"L'an deux mille vingt et un, le 28 octobre à 18 h 00, les conseillers communautaires de Roannais Agglomération, se sont réunis à l\\x92Espace Chorum \\x96 Halle\",\n",
              " 'Vacheresse \\x96',\n",
              " 'Rue des Vernes à Roanne.\\n\\n\\n\\n',\n",
              " 'La convocation de tous les conseillers a été faite le 22 octobre 2021, dans les formes et délais prescrits par la loi, par Yves Nicolin, Président.\\n\\n\\n\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "sents[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# to reconstruct based on sentences, we can iterate through the list with a range and\n",
        "# concat every 10 sents into a single string\n",
        "\n",
        "rang_sentence_union = np.arange(start=0, stop=len(sents), step=10)\n",
        "# print(rang)\n",
        "print(\"Total number of sents: \", len(sents))\n",
        "print(\"Number of final chunks: \", len(rang_sentence_union))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CY9hPOfbsCL",
        "outputId": "0a5d517c-eac1-465d-acd4-fadc54eef3cd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sents:  913\n",
            "Number of final chunks:  92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = []\n",
        "i = 0\n",
        "\n",
        "# merging sentences based on the ranges\n",
        "while i+1 < len(rang_sentence_union):\n",
        "  start = rang_sentence_union[i]\n",
        "  stop = rang_sentence_union[i+1]\n",
        "  # print(start, stop)\n",
        "\n",
        "  subset_to_join = sents[start : stop]\n",
        "  sent_union = \" \".join(subset_to_join)\n",
        "\n",
        "  paragraph_info = {\"paragraph_union\": sent_union,\n",
        "                    \"start_range\": start,\n",
        "                    \"stop_range\": stop,\n",
        "                    \"list_sents\":subset_to_join}\n",
        "\n",
        "  paragraphs.append(paragraph_info)\n",
        "\n",
        "  i += 1\n",
        "\n",
        "# if the stop of the range comes before the last sentence, we take those final couple of sentences\n",
        "# and add them to the last sentence of the paragraph list\n",
        "\n",
        "if stop != len(sents):\n",
        "\n",
        "  subset_to_join = sents[stop : len(sents)]\n",
        "  final_sents = \" \".join(subset_to_join)\n",
        "  para_to_edit = paragraphs.pop(-1)\n",
        "  final_union = para_to_edit[\"paragraph_union\"] + \" \" + final_sents\n",
        "\n",
        "  para_to_edit[\"paragraph_union\"] =  final_union\n",
        "  para_to_edit[\"stop_range\"] = len(sents)\n",
        "  para_to_edit[\"list_sents\"].extend(subset_to_join)\n",
        "\n",
        "  paragraphs.append(para_to_edit)"
      ],
      "metadata": {
        "id": "_QMgODsdbf9-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Vc93a6PgdPl",
        "outputId": "576b056b-8fae-4b0d-b16a-0bbc88b8ba4c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'paragraph_union': '# Dotations aux amortissements et aux provisions : 47 000 \\x80\\n\\n\\n\\nAjustement provision pour dépréciation des actifs circulants (-26 k\\x80) et provision pour Compte Epargne Temps (73 k\\x80).\\n\\n\\n\\n# Virement à la section d\\x92investissement : 118 500 \\x80\\n\\n\\n\\n(autofinancement complémentaire)\\n\\n\\n\\n# Recettes :\\n\\n\\n\\n# Produits des services : -257 140 \\x80\\n\\n\\n\\n Il s\\x92agit principalement de la diminution des recettes du Nauticum en raison du COVID et de la mauvaise météo de cet été (-300 k\\x80) et des billetteries Pleine nature (-42,14 k\\x80) et culturelles (-41,47 k\\x80), d\\x92un complément pour les revenus des coupes de bois (59,07 k\\x80) et à l\\x92ajustement de refacturations (67,4 k\\x80).\\n\\n\\n\\n # Impôts et taxes : 3 600 \\x80\\n\\n\\n\\n Ajustement du reversement des taxes foncières par la commune de Mably sur les bâtiments situés dans la ZAC de Mably. \\n\\n\\n\\n # Subventions - Dotations : -45 060 \\x80\\n\\n\\n\\nAjustement',\n",
              " 'start_range': 900,\n",
              " 'stop_range': 913,\n",
              " 'list_sents': ['# Dotations aux amortissements et aux provisions :',\n",
              "  '47 000 \\x80\\n\\n\\n\\nAjustement provision pour dépréciation des actifs circulants (-26 k\\x80) et provision pour Compte Epargne Temps (73 k\\x80).\\n\\n\\n\\n#',\n",
              "  'Virement à la section d\\x92investissement : 118 500 \\x80\\n\\n\\n\\n(autofinancement complémentaire)\\n\\n\\n\\n#',\n",
              "  'Recettes :\\n\\n\\n\\n#',\n",
              "  'Produits des services :',\n",
              "  '-257 140 \\x80\\n\\n\\n\\n',\n",
              "  'Il s\\x92agit principalement de la diminution des recettes du Nauticum en raison du COVID et de la mauvaise météo de cet été (-300 k\\x80) et des billetteries Pleine nature (-42,14 k\\x80) et culturelles (-41,47 k\\x80), d\\x92un complément pour les revenus des coupes de bois (59,07 k\\x80) et à l\\x92ajustement de refacturations (67,4 k\\x80).\\n\\n\\n\\n',\n",
              "  '# Impôts et taxes : 3 600 \\x80\\n\\n\\n\\n',\n",
              "  'Ajustement du reversement des taxes foncières par la commune de Mably sur les bâtiments situés dans la ZAC de Mably.',\n",
              "  '\\n\\n\\n\\n',\n",
              "  '#',\n",
              "  'Subventions',\n",
              "  '- Dotations : -45 060 \\x80\\n\\n\\n\\nAjustement']}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a list of dictionaries, one for each constructed paragraph. So far, each paragraph has the union string, start and end indexes as well as the list of individual sentences that were joined."
      ],
      "metadata": {
        "id": "K7WlBNr2gm8c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_QjccTUBASV"
      },
      "source": [
        "## Getting the text embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc1xqf1_IC-t"
      },
      "source": [
        "NOTE: There is one important caveat. We are computing sentence embeddings. If we were to pass the whole doc, the attention mask for each token would look very different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2IsSD5IEm62"
      },
      "source": [
        "### Example for one piece of text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8grmXscv4gyg"
      },
      "outputs": [],
      "source": [
        "# dir(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oXyC3J8g4gyg"
      },
      "outputs": [],
      "source": [
        "tokenized_markdown = tokenizer(markdown_example, return_tensors=\"pt\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqfU1Njp_T0B",
        "outputId": "bffac6ae-a815-4542-bcbb-7f7d4725e691"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20468"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "len(tokenized_markdown[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mvm7hSEq4gyg"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  # first batch (only one since we are processing one doc)\n",
        "  # final token\n",
        "    embeddings = model(**tokenized_markdown)[0][:, 0].squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaO-j34G_OeG",
        "outputId": "9f690874-398d-4de4-e5ac-f94803ff4417"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([896])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfe2cySo_fGQ"
      },
      "source": [
        "We take the last token since it's the one which contains all the aggregate info?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "r9Dz0XGjCnOC"
      },
      "outputs": [],
      "source": [
        "# embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CZN2u5nI4gyh"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "normalized_embeddings = F.normalize(embeddings, p=2, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "75kj1dKJA4Uq"
      },
      "outputs": [],
      "source": [
        "# normalized_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDRe1uk9Es4x"
      },
      "source": [
        "### For a list of sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OFOd5PChE7i_"
      },
      "outputs": [],
      "source": [
        "def compute_embeddings(sentence):\n",
        "\n",
        "    tokenized_sentences = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**tokenized_sentences)[0][:, 0].squeeze(0) # to take out the unused dimension since we are not batching\n",
        "        # print(embeddings.shape)\n",
        "\n",
        "    normalized_embeddings = F.normalize(embeddings, p=2, dim=0)\n",
        "    detached_embeddings = normalized_embeddings.detach().cpu().numpy() # detached into cpu so that we can manipulate them for clustering\n",
        "\n",
        "    torch.cuda.empty_cache() # careful with running out of memory\n",
        "\n",
        "    return detached_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8_SapulEFWyk"
      },
      "outputs": [],
      "source": [
        "list_paras = [para[\"paragraph_union\"] for para in paragraphs]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for paras, para_dict in zip(list_paras, paragraphs):\n",
        "    para_dict[\"para_embedding\"] = compute_embeddings(paras)"
      ],
      "metadata": {
        "id": "cBZZuOCjix2e"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjxS_QG8GiTj",
        "outputId": "83cfc46e-afee-4b64-881a-619aa8f1d6d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(896,)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "paragraphs[0][\"para_embedding\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now there is an extra key for the embeddings that we will use for the clsutering."
      ],
      "metadata": {
        "id": "RmGM9H8VlrGF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSc1QSlJGloT"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "90kGMssB5sJq"
      },
      "outputs": [],
      "source": [
        "unsqueezeded_embeddings = [para_dict[\"para_embedding\"] for para_dict in paragraphs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "QolXy-Qc6MRu"
      },
      "outputs": [],
      "source": [
        "# unsqueezeded_embeddings[0:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9-Od8pg7Idk"
      },
      "source": [
        "### Example for one number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Z5bNPk2o7yHR"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "W5XFk-ac5OEL"
      },
      "outputs": [],
      "source": [
        "gm = GaussianMixture(n_components=4, random_state=42)\n",
        "clusters = gm.fit_predict(unsqueezeded_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Hy--nqom6Z_l"
      },
      "outputs": [],
      "source": [
        "# clusters # label assigned to each sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF4VmocF6lgg",
        "outputId": "0cbb3cbd-b498-434e-d180-a9bf52fc2244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5186369\n"
          ]
        }
      ],
      "source": [
        "sil_sc = silhouette_score(unsqueezeded_embeddings, clusters)\n",
        "print(sil_sc) # the closer to 1 the better (how similar is an object to its cluster compared to the other clusters)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clusters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T89-GhBlkiPU",
        "outputId": "ed95ccdf-cc62-4c1b-9e59-9fa459199c8d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 3, 0, 1, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0,\n",
              "       0, 0, 3, 0, 3, 3, 3, 3, 1, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT9ds5aY7Kdu"
      },
      "source": [
        "### Optimal number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reomAYOP7Okn",
        "outputId": "48de469f-eae9-41ea-c849-359308d24c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 4 5 6 7 8]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# initial number of clusters to try\n",
        "range_clusters = np.arange(start=3, stop=9, step=1)\n",
        "print(range_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQcSWEgK_4HA"
      },
      "source": [
        "We could compute jensenshannon distance as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "coeaWSg63QUT"
      },
      "outputs": [],
      "source": [
        "def cluster_n(n_clusters, embeddings):\n",
        "\n",
        "  gm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
        "  clusters = gm.fit_predict(embeddings)\n",
        "  sil_sc = silhouette_score(unsqueezeded_embeddings, clusters)\n",
        "\n",
        "  print(\"Number of clusters: \", n_clusters)\n",
        "  print(\"Score: \", sil_sc)\n",
        "  print()\n",
        "\n",
        "  return clusters, sil_sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8LKJOTk4ZDi",
        "outputId": "d391e4ee-76d5-4311-a970-ac0db4efed1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clusters:  3\n",
            "Score:  0.5307558\n",
            "\n",
            "Number of clusters:  4\n",
            "Score:  0.5186369\n",
            "\n",
            "Number of clusters:  5\n",
            "Score:  0.35501358\n",
            "\n",
            "Number of clusters:  6\n",
            "Score:  0.35773662\n",
            "\n",
            "Number of clusters:  7\n",
            "Score:  0.3963203\n",
            "\n",
            "Number of clusters:  8\n",
            "Score:  0.47093514\n",
            "\n"
          ]
        }
      ],
      "source": [
        "silhouette_scores = []\n",
        "clusters_labels = []\n",
        "for n_cluster in range_clusters:\n",
        "  clusters, sil_sc = cluster_n(n_cluster, unsqueezeded_embeddings)\n",
        "  silhouette_scores.append(sil_sc)\n",
        "  clusters_labels.append(clusters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLWh2rQz8RC2",
        "outputId": "05f13a74-d953-4a8d-b115-df7280fcf747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 0\n",
            "Optimal Number of Clusters 3\n"
          ]
        }
      ],
      "source": [
        "max = np.argmax(silhouette_scores)\n",
        "optimal_n = range_clusters[max]\n",
        "print(\"Index\", max)\n",
        "print(\"Optimal Number of Clusters\", optimal_n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TRylDnIF8lOy"
      },
      "outputs": [],
      "source": [
        "final_clusters = clusters_labels[max]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eIyvmFl9sdO"
      },
      "source": [
        "## Logging Information, concatinating the text relating to each cluster and recomputing embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clusters_ids = {f\"cluster_{cluster_id}\": {\"para_indexes\": [],\n",
        "                                          \"union_paras\": \"\"} for cluster_id in np.arange(0, optimal_n, 1)}\n",
        "clusters_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6q0c0Upr0ec",
        "outputId": "ed377a3d-ef8d-420f-e0f8-70a00f699322"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cluster_0': {'para_indexes': [], 'union_paras': ''},\n",
              " 'cluster_1': {'para_indexes': [], 'union_paras': ''},\n",
              " 'cluster_2': {'para_indexes': [], 'union_paras': ''}}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the cluster in the paragraphs dictionary\n",
        "# and the indexes of the paragraphs of cluster in another one\n",
        "\n",
        "i = 0\n",
        "\n",
        "for para_dict, cluster in zip(paragraphs, final_clusters):\n",
        "  cluster_n_string = f\"cluster_{cluster}\"\n",
        "  para_dict[\"para_cluster\"] = cluster_n_string\n",
        "  clusters_ids[cluster_n_string][\"para_indexes\"].append(i)\n",
        "  clusters_ids[cluster_n_string][\"union_paras\"] = clusters_ids[cluster_n_string][\"union_paras\"] + para_dict[\"paragraph_union\"]\n",
        "  i = i + 1"
      ],
      "metadata": {
        "id": "z0bvr7rLlh9R"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# depending on the post-processing techniques, we may not be using these embeddings\n",
        "# and we'll have to re-tokenize and embed with the model we use\n",
        "\n",
        "for cluster in clusters_ids.keys():\n",
        "  text = clusters_ids[cluster][\"union_paras\"]\n",
        "  cluster_embds = compute_embeddings(text)\n",
        "  clusters_ids[cluster][\"cluster_embedding\"] = cluster_embds"
      ],
      "metadata": {
        "id": "eNH8LVu-0DAy"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clusters_ids[\"cluster_0\"]\n",
        "# looking at this example, we can see that the clustering is mostly sequential"
      ],
      "metadata": {
        "id": "c8VHpeFQsDjH"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JbkudLLndDi",
        "outputId": "69eb8213-17c6-444b-bff0-01bd0f55bd45"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['paragraph_union', 'start_range', 'stop_range', 'list_sents', 'para_embedding', 'para_cluster'])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clusters_ids.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3ioP6aysXKk",
        "outputId": "67205a2b-a824-40c3-987f-c9d36bcb1f75"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['cluster_0', 'cluster_1', 'cluster_2'])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clusters_ids[\"cluster_0\"].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IROnueIHz1X4",
        "outputId": "cfae2080-ca03-482f-bfd1-2d3a5b8f1804"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['para_indexes', 'union_paras', 'cluster_embedding'])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cPNekAS_KqF"
      },
      "source": [
        "RECAP\n",
        "\n",
        "So far we have:\n",
        "- the reconstructed text based on the paragraph clusters.\n",
        "- the embeddings of the reconstructed clusters.\n",
        "- the reconstructed paragraphs in ranges of 10 sentences.\n",
        "- the embeddings of the paragraphs detached and squeezed.\n",
        "- the cluster each paragraph belongs to.\n",
        "- the indexes of the paragraphs of each cluster.\n",
        "- the actual sentences split from SpaCy, in case we need them after.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEFT TO DO FOR THE PREPROCESS: GET THE NUMBER/TABLES"
      ],
      "metadata": {
        "id": "Sea9FktX1Vk2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQcVT6oq5E5R"
      },
      "source": [
        "## Sumarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQfcKOHADIRo"
      },
      "source": [
        "Now that we have the basic separation wrt to clusters, we can start creating different representations for each cluster and doing the hierarchical indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poc255oq5Gri"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "summ_model = 'csebuetnlp/mT5_multilingual_XLSum' # take the one for mt5 french!!\n",
        "\n",
        "tokenizer_summ = AutoTokenizer.from_pretrained(summ_model)\n",
        "model_summ = AutoModelForSeq2SeqLM.from_pretrained(summ_model).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRtbKqB19P-3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip())) # was part of the docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXOhgnCIAeJi"
      },
      "source": [
        "### Individual example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUvl3Ogu6l8-"
      },
      "outputs": [],
      "source": [
        "example_cluster = \" \".join(cluster_aggregates[5][\"texts\"])\n",
        "example_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iao1G9ap983n"
      },
      "outputs": [],
      "source": [
        "tokenized_example = tokenizer_summ(\n",
        "                              [WHITESPACE_HANDLER(example_cluster)],\n",
        "                              return_tensors=\"pt\",\n",
        "                              padding=\"max_length\",\n",
        "                              truncation=True\n",
        "                          ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAfEt4BU-ERE"
      },
      "outputs": [],
      "source": [
        "tokenized_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgFcC-EI-eUB"
      },
      "outputs": [],
      "source": [
        "input_ids = tokenized_example[\"input_ids\"]\n",
        "# input_ids = tokenized_example[\"input_ids\"].squeeze(dim=0) NOP,expects the unsqueexed dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5_KUYMu-kSn"
      },
      "outputs": [],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At4p5Mn8_pMP"
      },
      "outputs": [],
      "source": [
        "len(input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY34ss9r9W-P"
      },
      "outputs": [],
      "source": [
        "output = model_summ.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=1000,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_beams=4) # if it is too slow, adjust num_beams and max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPFNUWGB-8Mp"
      },
      "outputs": [],
      "source": [
        "output[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBfLlqnD-M6b"
      },
      "outputs": [],
      "source": [
        "summary = tokenizer_summ.decode(\n",
        "    output[0],\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSHTZ2Th9o3X"
      },
      "outputs": [],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKovfmQhAh53"
      },
      "source": [
        "### For the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE3clzofFqwl"
      },
      "outputs": [],
      "source": [
        "from torch.amp import autocast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMOBna6_5eFS"
      },
      "outputs": [],
      "source": [
        "def generate_summary(concat_text):\n",
        "\n",
        "  WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
        "\n",
        "  tokenized_example = tokenizer_summ(\n",
        "                              [WHITESPACE_HANDLER(concat_text)],\n",
        "                              return_tensors=\"pt\",\n",
        "                              padding=\"max_length\",\n",
        "                              truncation=True).to(device)\n",
        "\n",
        "  input_ids = tokenized_example[\"input_ids\"]\n",
        "\n",
        "  model_summ.eval()\n",
        "  with autocast(\"cuda\"):\n",
        "    summary_embedding_output = model_summ.generate(\n",
        "                                              input_ids=input_ids,\n",
        "                                              max_length=1000,\n",
        "                                              no_repeat_ngram_size=2,\n",
        "                                              num_beams=4) # if it is too slow, adjust num_beams and max_len\n",
        "\n",
        "  summary_decoded = tokenizer_summ.decode(\n",
        "                                    summary_embedding_output[0],\n",
        "                                    skip_special_tokens=True,\n",
        "                                    clean_up_tokenization_spaces=False)\n",
        "\n",
        "  torch.cuda.empty_cache() # Careful with running out of memory\n",
        "\n",
        "  return summary_embedding_output.detach().cpu().numpy(), summary_decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT5TmgOkEZBj"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RmBpsqmDgES"
      },
      "outputs": [],
      "source": [
        "def process_cluster(cluster, cluster_aggregates=cluster_aggregates):\n",
        "\n",
        "  print(\"Procesing cluster \", cluster+1)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  cluster_aggregates[cluster][\"broader_representations\"] = {}\n",
        "\n",
        "  concat_text = \" \".join(cluster_aggregates[cluster][\"texts\"])\n",
        "  cluster_aggregates[cluster][\"broader_representations\"][\"concat_text\"] = concat_text\n",
        "\n",
        "\n",
        "  cluster_embeddings = compute_embeddings(concat_text)\n",
        "  cluster_aggregates[cluster][\"broader_representations\"][\"cluster_sentence_embeddings\"] = cluster_embeddings[0] # taking out the extra unnecesary dimension\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  summary_embedding_output, summary_decoded = generate_summary(concat_text)\n",
        "  cluster_aggregates[cluster][\"broader_representations\"][\"cluster_summary_embedding\"] = summary_embedding_output\n",
        "  cluster_aggregates[cluster][\"broader_representations\"][\"cluster_summary_decoded\"] = summary_decoded\n",
        "\n",
        "  return cluster_aggregates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aapNTjqsFXs7"
      },
      "outputs": [],
      "source": [
        "for cluster in list(cluster_aggregates.keys()):\n",
        "  cluster_aggregates = process_cluster(cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of12WNBcWAj0"
      },
      "outputs": [],
      "source": [
        "cluster_aggregates[1][\"broader_representations\"].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5iE6NkocBDS"
      },
      "outputs": [],
      "source": [
        "cluster_aggregates[1][\"broader_representations\"][\"cluster_summary_decoded\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TET1QZvihmSF"
      },
      "outputs": [],
      "source": [
        "pprint(cluster_aggregates[1][\"broader_representations\"][\"concat_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sS5NuQk_choX"
      },
      "outputs": [],
      "source": [
        "cluster_aggregates[0][\"broader_representations\"][\"cluster_summary_decoded\"]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}