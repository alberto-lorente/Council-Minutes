{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['doc_id', 'url', 'cache', 'fulltext', 'nature', 'published',\n",
       "       'entity_name', 'entity_type', 'geo_path', 'extracted_text',\n",
       "       'embeddings', 'typed_embeddings', 'km_cluster_labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data_annotated.csv\")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(np.unique(data[\"nature\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_french(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df, category):\n",
    "    \n",
    "    df.dropna(inplace = True)\n",
    "    df = df[df[\"nature\"] == category]\n",
    "    \n",
    "    n_samples = len(df)\n",
    "    \n",
    "    return n_samples, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf(X_train, X_test):\n",
    "    \n",
    "    tfidf = TfidfVectorizer(lowercase=True, stop_words=stop_words, max_features=200, tokenizer=tokenize_french)\n",
    "    \n",
    "    X_tf_train = tfidf.fit_transform(X_train)\n",
    "    X_tf_test = tfidf.transform(X_test)\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    \n",
    "    return X_tf_train, X_tf_test, feature_names, tfidf\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_cluster_tfidf(df, category):\n",
    "    \n",
    "    n_samples, df = filter_data(df, category)\n",
    "    if n_samples < 30:\n",
    "        return \"not enough data\"\n",
    "\n",
    "    X = df[\"extracted_text\"]\n",
    "    y = df[\"km_cluster_labels\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    \n",
    "    X_tf_train, X_tf_test, feature_names, tfidf = create_tfidf(X_train, X_test)\n",
    "    \n",
    "    rf = RandomForestClassifier(criterion=\"entropy\", random_state=42)\n",
    "    rf.fit(X_tf_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_tf_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"RF f1-score:\", f1)\n",
    "    \n",
    "    importances = rf.feature_importances_\n",
    "    \n",
    "    dict_feature_importance_tfidf = dict(zip(feature_names, importances))\n",
    "    with open(f\"{category}_explainability_tfidf.json\", \"w\") as f:\n",
    "        json.dump(dict_feature_importance_tfidf, f)\n",
    "\n",
    "    return dict_feature_importance_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['avion', 'avoir', 'eussion', 'eûte', 'fussion', 'fûte', 'luire', 'éter', 'être'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF f1-score: 0.8888888888888888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['avion', 'avoir', 'eussion', 'eûte', 'fussion', 'fûte', 'luire', 'éter', 'être'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF f1-score: 0.9151670951156813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['avion', 'avoir', 'eussion', 'eûte', 'fussion', 'fûte', 'luire', 'éter', 'être'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF f1-score: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['avion', 'avoir', 'eussion', 'eûte', 'fussion', 'fûte', 'luire', 'éter', 'être'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF f1-score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['avion', 'avoir', 'eussion', 'eûte', 'fussion', 'fûte', 'luire', 'éter', 'être'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF f1-score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['avion', 'avoir', 'eussion', 'eûte', 'fussion', 'fûte', 'luire', 'éter', 'être'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF f1-score: 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\alber\\Desktop\\envs\\.deep-env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['avion', 'avoir', 'eussion', 'eûte', 'fussion', 'fûte', 'luire', 'éter', 'être'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for category in categories:\n",
    "    dict_feature_importance_tfidf = explain_cluster_tfidf(data, category)\n",
    "    results.append(dict_feature_importance_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acte.arrete_explainability_tfidf.json', 'acte.delib_explainability_tfidf.json', 'acte.raa_explainability_tfidf.json', 'bdj_explainability_tfidf.json', 'comm_explainability_tfidf.json', 'dlao.autres_explainability_tfidf.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cf = os.listdir()\n",
    "list_files = [file_ for file_ in cf if \".json\" in file_]\n",
    "print(list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_file = \"acte.arrete_explainability_tfidf.json\"\n",
    "\n",
    "with open(json_file, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=1, step=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([data]).T\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=0, ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['région', 'direction', 'subdélégation', 'recueil', 'agrément',\n",
       "       'relatif', 'mission', 'transport', 'arrêter', 'mois'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_results = []\n",
    "top_20_per_cat = []\n",
    "for json_file in list_files:\n",
    "    # print(json_file)\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    data_results.append(data)\n",
    "    \n",
    "    df = pd.DataFrame([data]).T\n",
    "    df.sort_values(by=0, ascending=False, inplace=True)\n",
    "    top_20 = list(df.head(20).index)\n",
    "    dict_doc_results = {\"doc_type\": json_file.replace(\"_explainability_tfidf.json\", \"\"),\n",
    "                        \"important_words\": top_20}\n",
    "    top_20_per_cat.append(dict_doc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_type': 'acte.arrete',\n",
       "  'important_words': ['région',\n",
       "   'direction',\n",
       "   'subdélégation',\n",
       "   'recueil',\n",
       "   'agrément',\n",
       "   'relatif',\n",
       "   'mission',\n",
       "   'transport',\n",
       "   'arrêter',\n",
       "   'mois',\n",
       "   'département',\n",
       "   'décret',\n",
       "   'national',\n",
       "   'devoir',\n",
       "   'e',\n",
       "   'être',\n",
       "   'signer',\n",
       "   'maire',\n",
       "   'faire',\n",
       "   'délai']},\n",
       " {'doc_type': 'acte.delib',\n",
       "  'important_words': ['séance',\n",
       "   'délibération',\n",
       "   'municipal',\n",
       "   'conseil',\n",
       "   'présent',\n",
       "   'Monsieur',\n",
       "   'avoir',\n",
       "   'être',\n",
       "   'id',\n",
       "   'monsieur',\n",
       "   'exercice',\n",
       "   'préfecture',\n",
       "   'maire',\n",
       "   'code',\n",
       "   'recevoir',\n",
       "   'publier',\n",
       "   'voir',\n",
       "   'envoyer',\n",
       "   'relatif',\n",
       "   'après']},\n",
       " {'doc_type': 'acte.raa',\n",
       "  'important_words': ['Monsieur',\n",
       "   'faire',\n",
       "   'sou',\n",
       "   'présent',\n",
       "   'article',\n",
       "   'ville',\n",
       "   'général',\n",
       "   'ci',\n",
       "   'travail',\n",
       "   'municipal',\n",
       "   'autre',\n",
       "   'police',\n",
       "   'objet',\n",
       "   'droit',\n",
       "   'total',\n",
       "   'activité',\n",
       "   'étude',\n",
       "   'national',\n",
       "   'arrêté',\n",
       "   'loi']},\n",
       " {'doc_type': 'bdj',\n",
       "  'important_words': ['local',\n",
       "   'avoir',\n",
       "   'fonds',\n",
       "   'travail',\n",
       "   'investissement',\n",
       "   'compte',\n",
       "   'service',\n",
       "   'être',\n",
       "   'opération',\n",
       "   'personnel',\n",
       "   'subvention',\n",
       "   'public',\n",
       "   'recette',\n",
       "   'général',\n",
       "   'année',\n",
       "   'entrer',\n",
       "   'devoir',\n",
       "   'dépense',\n",
       "   'budget',\n",
       "   'charge']},\n",
       " {'doc_type': 'comm',\n",
       "  'important_words': ['avoir',\n",
       "   'être',\n",
       "   'tout',\n",
       "   'projet',\n",
       "   'travail',\n",
       "   'public',\n",
       "   'faire',\n",
       "   'permettre',\n",
       "   'pouvoir',\n",
       "   'plus',\n",
       "   'énergie',\n",
       "   'entrer',\n",
       "   'service',\n",
       "   'place',\n",
       "   'local',\n",
       "   'commune',\n",
       "   'nouveau',\n",
       "   'mettre',\n",
       "   'territoire',\n",
       "   'depuis']},\n",
       " {'doc_type': 'dlao.autres',\n",
       "  'important_words': ['e',\n",
       "   'règlement',\n",
       "   'mettre',\n",
       "   'immeuble',\n",
       "   'porter',\n",
       "   'qualité',\n",
       "   'avoir',\n",
       "   'lier',\n",
       "   'applicable',\n",
       "   'arrêté',\n",
       "   'article',\n",
       "   'ensemble',\n",
       "   'règle',\n",
       "   'concerner',\n",
       "   'préfecture',\n",
       "   'public',\n",
       "   'permettre',\n",
       "   'supérieur',\n",
       "   'terrain',\n",
       "   'mise']}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20_per_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acte.arrete\n",
      "région, direction, subdélégation, recueil, agrément, relatif, mission, transport, arrêter, mois, département, décret, national, devoir, e, être, signer, maire, faire, délai\n",
      "acte.delib\n",
      "séance, délibération, municipal, conseil, présent, Monsieur, avoir, être, id, monsieur, exercice, préfecture, maire, code, recevoir, publier, voir, envoyer, relatif, après\n",
      "acte.raa\n",
      "Monsieur, faire, sou, présent, article, ville, général, ci, travail, municipal, autre, police, objet, droit, total, activité, étude, national, arrêté, loi\n",
      "bdj\n",
      "local, avoir, fonds, travail, investissement, compte, service, être, opération, personnel, subvention, public, recette, général, année, entrer, devoir, dépense, budget, charge\n",
      "comm\n",
      "avoir, être, tout, projet, travail, public, faire, permettre, pouvoir, plus, énergie, entrer, service, place, local, commune, nouveau, mettre, territoire, depuis\n",
      "dlao.autres\n",
      "e, règlement, mettre, immeuble, porter, qualité, avoir, lier, applicable, arrêté, article, ensemble, règle, concerner, préfecture, public, permettre, supérieur, terrain, mise\n"
     ]
    }
   ],
   "source": [
    "for results in top_20_per_cat:\n",
    "    words = results[\"important_words\"]\n",
    "    string = \", \".join(words)\n",
    "    print(results['doc_type'])\n",
    "    print(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".deep-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
