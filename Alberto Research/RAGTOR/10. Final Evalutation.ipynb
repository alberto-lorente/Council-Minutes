{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"HF_TOKEN.txt\", \"r\") as f:\n",
    "    hf_token = f.read()\n",
    "\n",
    "with open(\"GROQ_KEY.txt\", \"r\") as f:\n",
    "    groq_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alberto-lorente\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder, whoami\n",
    "\n",
    "HfFolder.save_token(hf_token)\n",
    "print(whoami()[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda available\")\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "eval_path = r\"Evaluation data\\Solved PDFs\"\n",
    "solved_pdfs_path = os.listdir(eval_path)\n",
    "solved_pdfs_path_pdf_files = [os.path.join(eval_path, pdf) for pdf in solved_pdfs_path if pdf.endswith(\"pdf\")]\n",
    "solved_pdfs_path_txt_files = [os.path.join(eval_path, txt) for txt in solved_pdfs_path if txt.endswith(\"txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tuple_data = []\n",
    "i = 0\n",
    "while i < len(solved_pdfs_path_pdf_files):\n",
    "    tup = (solved_pdfs_path_pdf_files[i], solved_pdfs_path_txt_files[i])\n",
    "    list_tuple_data.append(tup)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Evaluation data\\\\Solved PDFs\\\\4b7076dc7989552fc75046c32d068eee9122adff_DEL13-1223-Annex.pdf',\n",
       "  'Evaluation data\\\\Solved PDFs\\\\4b7076dc7989552fc75046c32d068eee9122adff_DEL13-1223-Annex.txt'),\n",
       " ('Evaluation data\\\\Solved PDFs\\\\7a3ef5c116cca7f5328c44ef2c3f227f8c90ef60_Zones_Accélérati.pdf',\n",
       "  'Evaluation data\\\\Solved PDFs\\\\7a3ef5c116cca7f5328c44ef2c3f227f8c90ef60_Zones_Accélérati.txt'),\n",
       " ('Evaluation data\\\\Solved PDFs\\\\a7f4c9d0d7c8f3c9cd67b7c80beec26d9c234382_delibs-22-mai-24.pdf',\n",
       "  'Evaluation data\\\\Solved PDFs\\\\a7f4c9d0d7c8f3c9cd67b7c80beec26d9c234382_delibs-22-mai-24.txt'),\n",
       " ('Evaluation data\\\\Solved PDFs\\\\ecc6f_07-12-juillet.pdf',\n",
       "  'Evaluation data\\\\Solved PDFs\\\\ecc6f_07-12-juillet.txt'),\n",
       " ('Evaluation data\\\\Solved PDFs\\\\fa224c7892c9c4971dda6423c75c97a04f6e3666_del2024_22-defin.pdf',\n",
       "  'Evaluation data\\\\Solved PDFs\\\\fa224c7892c9c4971dda6423c75c97a04f6e3666_del2024_22-defin.txt')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tuple_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"prompts_preprocessing.json\", \"r\") as f:\n",
    "    prompts_for_processing = json.load(f)\n",
    "\n",
    "table_process_prompt = prompts_for_processing[\"augment_table_prompt\"]\n",
    "summary_prompt = prompts_for_processing[\"summary_prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"french_prompts.json\", \"r\") as f:\n",
    "    prompts_query = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = list_tuple_data[0][0]\n",
    "markdown = list_tuple_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from council_rag.preprocessing import preprocess_markdown_text\n",
    "from council_rag.data_transformations import process_tables, summarize_clusters\n",
    "from council_rag.preprocessing.preprocessing import unload_cuda\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "paragraphs_list, clusters_dict, model = preprocess_markdown_text(markdown,\n",
    "                                                        model_id =\"Jaume/gemma-2b-embeddings\", \n",
    "                                                        spacy_model=\"fr_core_news_sm\", \n",
    "                                                        n_sents_per_para=8,\n",
    "                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_cuda()\n",
    "processed_tables = process_tables(pdf_path, \n",
    "                                table_process_prompt, \n",
    "                                groq_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_cuda()\n",
    "clusters_dict = summarize_clusters(clusters_dict, \n",
    "                                    summary_prompt, \n",
    "                                    groq_token, \n",
    "                                    model=\"gemma2-9b-it\", \n",
    "                                    token_limit=14000, \n",
    "                                    sleep_time=60)\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from council_rag.rag import prepare_data_for_rag\n",
    "\n",
    "unload_cuda()\n",
    "clusters_list, paragraphs_list, all_docs = prepare_data_for_rag(clusters_dict, \n",
    "                                                                paragraphs_list, \n",
    "                                                                processed_tables, \n",
    "                                                                splitter=RecursiveCharacterTextSplitter,\n",
    "                                                                chunk_size=450,\n",
    "                                                                chunk_overlap=35,\n",
    "                                                                length_function=len,\n",
    "                                                                is_separator_regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'generate_groq_summary' from 'council_rag.preprocessing' (c:\\Users\\alber\\Desktop\\Council-Minutes\\Alberto Research\\RAGTOR\\council_rag\\preprocessing\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcouncil_rag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m shorten_summary_docs\n\u001b[0;32m      3\u001b[0m unload_cuda()\n\u001b[0;32m      4\u001b[0m all_docs \u001b[38;5;241m=\u001b[39m shorten_summary_docs(all_docs, groq_token, model)\n",
      "File \u001b[1;32mc:\\Users\\alber\\Desktop\\Council-Minutes\\Alberto Research\\RAGTOR\\council_rag\\rag\\__init__.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcouncil_rag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_groq_summary\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfFolder, whoami\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_md_to_text.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f: \u001b[38;5;66;03m# TO DO: check proper encoding for .md files\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'generate_groq_summary' from 'council_rag.preprocessing' (c:\\Users\\alber\\Desktop\\Council-Minutes\\Alberto Research\\RAGTOR\\council_rag\\preprocessing\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from council_rag.rag import shorten_summary_docs\n",
    "\n",
    "unload_cuda()\n",
    "all_docs = shorten_summary_docs(all_docs, groq_token, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
